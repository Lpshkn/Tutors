{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a5064ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240e6dab",
   "metadata": {},
   "source": [
    "# Загрузка и препроцессинг данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7827b05b",
   "metadata": {},
   "source": [
    "Нейросети не работают с сырыми даными по типу JPEG или CSV. Вместо этого они оперируют **векторами** и **специальными** представлениями:\n",
    "* Текстовые файлы должны быть прочитаны, строки получены и разбиты на слова. Слова должны быть проиндексированы и превращены в целочисленные тензоры.\n",
    "* Изображения должны быть так же прочитаны и преобразованы в тензоры пикселей (целочисленные), а затем нормализованы до некоторых маленьких значений (часто 0-1).\n",
    "* Табличные данные должны быть обработаны: категоризованные признаки должны быть приведены в целочисленные тензоры, а признаки в виде чисел с плавающей точкой преобразованы в соответствующие тензоры с плавающей точкой. В конце все признаки точно так же должны быть нормализованы, т.к. этого требуют модели.\n",
    "* Ну и т.д."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cb0c0f",
   "metadata": {},
   "source": [
    "## Загрузка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1ae1e6",
   "metadata": {},
   "source": [
    "Собственно, Keras оперирует тремя типами \"входов\" (или входных данных):\n",
    "* NumPy массивами (те самые `ndarray`), точно так же, как это делает sklearn. Это хороший вариант, если все данные умещаются в RAM.\n",
    "* Tensorflow Dataset объекты. Это вариант подходит для тех случаев, когда данные не помещаются в память и будут считывать либо с диска, либо вообще с распределенной ФС.\n",
    "* Питоновские генераторы, которые отдают пакеты данных (очевидно,  всё, что реализовано как генератор, может передаваться в Keras)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa52c24",
   "metadata": {},
   "source": [
    "Важно отметить, что если датасет правда большой, а вы собираетесь обучать модель на GPU, то стоит использовать объект Tensorflow Dataset, т.к. он обеспечивает высокую производительность засчёт:\n",
    "* ассинхронной предобработки данных на CPU, пока GPU загружена, и постановка предобработанных данных в очередь\n",
    "* поставки данных в GPU память так, что она немедленно становится доступна для GPU в момент, когда он прекратил обработку предыдущего поднабора данных. Это позволяет использовать GPU на полную."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e74d5f",
   "metadata": {},
   "source": [
    "Keras позволяет использовать ряд утилит для загрузки сырых данных с диска и превращения их сразу в объект `Dataset`:\n",
    "* `tf.keras.preprocessing.image_dataset_from_directory` - для превращения изображений, отсортированных по специальной классовой структуре (в виде структуры папок) в размеченный датасет тензоров изображений.\n",
    "* `tf.keras.preprocessing.text_dataset_from_directory` - то же самое, но для текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2bc3ce",
   "metadata": {},
   "source": [
    "Как пример просто загрузим [вот этот датасет](https://www.kaggle.com/olgabelitskaya/tomato-cultivars) и распределим его по директориям (каждая директория представляет собой отдельный конкретный класс). Теперь можем воспользоваться методом выше, чтобы автоматически создать размеченный датасет из структуры папок.\n",
    "\n",
    "Моя структура папок:\n",
    "```\n",
    "Tomato Cultivars/\n",
    "...tomato_1/\n",
    "......01_001.png\n",
    "......01_002.png\n",
    "................\n",
    "...tomato_2/\n",
    "......02_001.png\n",
    "......02_002.png\n",
    "................\n",
    "............\n",
    "...tomato_15/\n",
    "......15_001.png\n",
    "......15_002.png\n",
    "................\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a854556f",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_directory = '/home/lpshkn/Datasets/Tomato Cultivars/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "498965dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 776 files belonging to 15 classes.\n"
     ]
    }
   ],
   "source": [
    "dataset = keras.preprocessing.image_dataset_from_directory(main_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57c2f33a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tomato_1',\n",
       " 'tomato_10',\n",
       " 'tomato_11',\n",
       " 'tomato_12',\n",
       " 'tomato_13',\n",
       " 'tomato_14',\n",
       " 'tomato_15',\n",
       " 'tomato_2',\n",
       " 'tomato_3',\n",
       " 'tomato_4',\n",
       " 'tomato_5',\n",
       " 'tomato_6',\n",
       " 'tomato_7',\n",
       " 'tomato_8',\n",
       " 'tomato_9']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58324da",
   "metadata": {},
   "source": [
    "Как можно заметить, это идеально подходит для случаев, когда данных в виде текстовых или изображений очень много и все они разбросаны по директориям. Тогда можно написать простой bash скрипт, уложив всё по папкам, и использовать эти функции."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65f620b",
   "metadata": {},
   "source": [
    "## Предобработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d391fadf",
   "metadata": {},
   "source": [
    "Как только ваши данные приобрели вид строковых/числовых Numpy массивов или `Dataset` объекта (или питоновского генератора), то это значит, что такой объект способен \"выдавать\" порции данных в виде строковых/числовых тензоров. Теперь настолько время **предобработки** данных:\n",
    "* Токенизация строковых данных с последующей индексацией токенов\n",
    "* Масштабирование признаков (нормализация)\n",
    "* Приведение данных к значениям близким к нулю (этого требуют нейросети - обычно данные должны быть с 0-м мат.ожиданием и 1-чной дисперсией, либо в диапазоне [0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a703580",
   "metadata": {},
   "source": [
    "Мы должны стремиться к тому, чтобы препроцессинг данных осуществлялся **прямо в модели**, а не в виде какого-то внешнего пайплайна. Всё потому, что внешняя предобработка делает нашу модель менее переносимой, особенно, когда она должна использоваться прямо в продакшне.\n",
    "\n",
    "Собственно, если ваша модель портируется на какое-то мобильное приложение, а вам надо предобработать данные на конкретном языке, то вы должны учесть, что язык может поменяться. В таком случае, любое несоответствие с исходным предположением о данных, подаваемых модели, полностью сломает нам всю систему.\n",
    "\n",
    "Поэтому, куда проще просто экспортировать модель (end-to-end), которая уже включает в себя препроцессинг. **Идеальной** моделью можем считать такую модель, которая на вход получает максимально близкие к сырым данные: т.е. изображения могут быть RGB с интенсивностью в диапазоне [0, 255], а тексты - в виде строк формата `utf-8`. По сути, это очень облегчает жизнь пользователю модели - он понятия не имеет о том, как проходит препроцессинг внутри."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77451f7",
   "metadata": {},
   "source": [
    "### Слои предобработки Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce5b28e",
   "metadata": {},
   "source": [
    "Такой препроцессинг можно осуществить с помощью **слоёв предобработки**:\n",
    "* Векторизацию строк текстов через слой `TextVectorization`\n",
    "* Нормализацию признаков через слой `Normalization`\n",
    "* Также они позволяют выполнить масштабирование изображений, обрезание данных или даже процесс увеличения датасета изображений (image data augmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d96ef64",
   "metadata": {},
   "source": [
    "Все эти слои позволяют сделать вашу модель **переносимой**.\n",
    "\n",
    "Слой препроцессинга запускается вызовом `layer.adapt(data)` на выборке данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "878f1885",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b10f0ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(['Мама мыла раму очень качественно.', \n",
    "                    'Я могу только сказать, что вы сильно заблуждаетесь.',\n",
    "                    'Я могу купить этот дом, но со временем передумали.',\n",
    "                    'Он достиг небывалых высот в своём деле - это достойно!']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "083cd691",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TextVectorization(output_mode='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60bd8c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.adapt(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b76bb306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 9), dtype=int64, numpy=\n",
       "array([[30, 17, 12, 14, 19,  0,  0,  0,  0],\n",
       "       [ 3,  2,  7,  9,  6, 26, 10, 20,  0],\n",
       "       [ 3,  2, 18,  4, 23, 15,  8, 27, 13],\n",
       "       [29, 22, 16, 25, 28, 11, 24,  5, 21]])>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d39e581a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'могу', 'Я', 'этот', 'это', 'что', 'только', 'со', 'сказать', 'сильно', 'своём', 'раму', 'передумали', 'очень', 'но', 'небывалых', 'мыла', 'купить', 'качественно', 'заблуждаетесь', 'достойно', 'достиг', 'дом', 'деле', 'высот', 'вы', 'временем', 'в', 'Он', 'Мама']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf07168",
   "metadata": {},
   "source": [
    "Когда мы вызываем метод `adapt`, он обрабатывает переданные данные и создаёт \"словарь\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3961b50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
